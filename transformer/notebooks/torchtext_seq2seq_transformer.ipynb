{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Modeling With nn.Transformer and TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "from src.config import *\n",
    "from src.models import TransformerModel\n",
    "from src.train_model import train\n",
    "from src.evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orsdanilo/projects/personal/pytorch-tutorials/env/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 322.66 | loss  5.57 | ppl   261.80\n",
      "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 335.55 | loss  5.70 | ppl   298.53\n",
      "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 329.63 | loss  5.54 | ppl   255.08\n",
      "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 316.98 | loss  5.60 | ppl   269.50\n",
      "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 318.27 | loss  5.56 | ppl   258.75\n",
      "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 322.79 | loss  5.60 | ppl   270.56\n",
      "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 318.16 | loss  5.62 | ppl   274.99\n",
      "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 319.96 | loss  5.65 | ppl   283.90\n",
      "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 320.28 | loss  5.57 | ppl   263.35\n",
      "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 322.99 | loss  5.61 | ppl   272.08\n",
      "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 349.55 | loss  5.50 | ppl   244.37\n",
      "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 351.98 | loss  5.57 | ppl   261.84\n",
      "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 359.91 | loss  5.57 | ppl   263.62\n",
      "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 357.54 | loss  5.51 | ppl   246.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1019.25s | valid loss  5.60 | valid ppl   271.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 346.69 | loss  5.40 | ppl   221.56\n",
      "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 357.17 | loss  5.50 | ppl   244.80\n",
      "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 358.11 | loss  5.33 | ppl   207.39\n",
      "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 340.47 | loss  5.40 | ppl   222.43\n",
      "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 313.94 | loss  5.37 | ppl   215.16\n",
      "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 324.18 | loss  5.40 | ppl   221.67\n",
      "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 335.69 | loss  5.43 | ppl   227.15\n",
      "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 315.14 | loss  5.47 | ppl   236.58\n",
      "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 320.82 | loss  5.40 | ppl   221.71\n",
      "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 339.88 | loss  5.43 | ppl   228.47\n",
      "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 328.36 | loss  5.32 | ppl   203.64\n",
      "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 315.43 | loss  5.40 | ppl   221.21\n",
      "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 318.57 | loss  5.42 | ppl   224.84\n",
      "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 328.09 | loss  5.37 | ppl   215.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 1010.92s | valid loss  5.52 | valid ppl   248.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 323.44 | loss  5.27 | ppl   194.58\n",
      "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 315.80 | loss  5.36 | ppl   213.27\n",
      "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 315.25 | loss  5.19 | ppl   179.30\n",
      "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 325.45 | loss  5.26 | ppl   192.09\n",
      "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 333.11 | loss  5.22 | ppl   185.75\n",
      "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 339.93 | loss  5.27 | ppl   194.03\n",
      "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 355.07 | loss  5.29 | ppl   198.43\n",
      "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 357.01 | loss  5.33 | ppl   207.08\n",
      "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 351.69 | loss  5.27 | ppl   195.00\n",
      "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 352.22 | loss  5.30 | ppl   200.41\n",
      "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 353.36 | loss  5.18 | ppl   178.29\n",
      "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 347.16 | loss  5.27 | ppl   193.59\n",
      "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 345.30 | loss  5.30 | ppl   199.99\n",
      "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 341.60 | loss  5.23 | ppl   186.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 1039.01s | valid loss  5.51 | valid ppl   246.37\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_data, optimizer, scheduler, criterion, epoch)\n",
    "    val_loss = evaluate(model, val_data, criterion)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model with the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.41 | test ppl   224.63\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data, criterion)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
